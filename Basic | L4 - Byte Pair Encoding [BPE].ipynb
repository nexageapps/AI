{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nexageapps/AI/blob/main/Basic%20%7C%20L3%20-%20Binary%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3mEFMociE0Av",
        "outputId": "1f7caa96-621b-4bf2-e482-f7c28d738b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Vocabulary (character-level):\n",
            "  'l o w </w>': 1\n",
            "  'l o w e r </w>': 1\n",
            "  'l o w e s t </w>': 1\n",
            "\n",
            "Performing 5 BPE merges...\n",
            "\n",
            "Merge 1: Merging ('l', 'o') -> 'lo' (frequency: 3)\n",
            "Merge 2: Merging ('lo', 'w') -> 'low' (frequency: 3)\n",
            "Merge 3: Merging ('low', 'e') -> 'lowe' (frequency: 2)\n",
            "Merge 4: Merging ('low', '</w>') -> 'low</w>' (frequency: 1)\n",
            "Merge 5: Merging ('lowe', 'r') -> 'lower' (frequency: 1)\n",
            "\n",
            "Final Vocabulary after BPE:\n",
            "  'low</w>': 1\n",
            "  'lower </w>': 1\n",
            "  'lowe s t </w>': 1\n",
            "\n",
            "==================================================\n",
            "Tokenization Comparison:\n",
            "==================================================\n",
            "\n",
            "Original word: 'lowest'\n",
            "Character-level tokens: ['l', 'o', 'w', 'e', 's', 't'] (6 tokens)\n",
            "After BPE tokens: ['lowe', 's', 't'] (3 tokens)\n",
            "BPE representation: 'lowe s t </w>'\n",
            "\n",
            "==================================================\n",
            "Key Insight:\n",
            "==================================================\n",
            "BPE learns subword units that balance between:\n",
            "  • Character-level: Flexible but creates LONG sequences\n",
            "  • Word-level: Compact but LIMITED vocabulary\n",
            "  • Subword-level (BPE): BEST OF BOTH WORLDS!\n",
            "\n",
            "This is why GPT and other modern LLMs use BPE tokenization.\n"
          ]
        }
      ],
      "source": [
        "# Author: Karthik Arjun\n",
        "# LinkedIn: https://www.linkedin.com/in/karthik-arjun-a5b4a258/\n",
        "# Comment: Beginner Level - Byte Pair Encoding [BPE]\n",
        "# Reference: https://github.com/openai/tiktoken\n",
        "# Date: 02/11/2025\n",
        "\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Simple Byte Pair Encoding (BPE) Implementation\n",
        "# BPE is a tokenization algorithm used in NLP models like GPT\n",
        "\n",
        "def get_stats(vocab):\n",
        "    \"\"\"Count frequency of adjacent character pairs\"\"\"\n",
        "    pairs = Counter()\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[symbols[i], symbols[i + 1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, vocab):\n",
        "    \"\"\"Merge the most frequent pair in vocabulary\"\"\"\n",
        "    new_vocab = {}\n",
        "    bigram = ' '.join(pair)\n",
        "    replacement = ''.join(pair)\n",
        "    \n",
        "    for word in vocab:\n",
        "        new_word = word.replace(bigram, replacement)\n",
        "        new_vocab[new_word] = vocab[word]\n",
        "    return new_vocab\n",
        "\n",
        "# Example: Simple text corpus\n",
        "text = \"low lower lowest\"\n",
        "\n",
        "# Step 1: Initialize vocabulary with character-level tokens\n",
        "vocab = {}\n",
        "for word in text.split():\n",
        "    # Add space between characters and end-of-word marker\n",
        "    word_tokens = ' '.join(list(word)) + ' </w>'\n",
        "    vocab[word_tokens] = vocab.get(word_tokens, 0) + 1\n",
        "\n",
        "print(\"Initial Vocabulary (character-level):\")\n",
        "for word, freq in vocab.items():\n",
        "    print(f\"  '{word}': {freq}\")\n",
        "\n",
        "# Step 2: Perform BPE merges\n",
        "num_merges = 5\n",
        "print(f\"\\nPerforming {num_merges} BPE merges...\\n\")\n",
        "\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    \n",
        "    # Find most frequent pair\n",
        "    best_pair = max(pairs, key=pairs.get)\n",
        "    print(f\"Merge {i+1}: Merging ('{best_pair[0]}', '{best_pair[1]}') -> '{best_pair[0]}{best_pair[1]}' (frequency: {pairs[best_pair]})\")\n",
        "    \n",
        "    # Merge the pair\n",
        "    vocab = merge_vocab(best_pair, vocab)\n",
        "\n",
        "print(\"\\nFinal Vocabulary after BPE:\")\n",
        "for word, freq in vocab.items():\n",
        "    print(f\"  '{word}': {freq}\")\n",
        "\n",
        "# Demonstrate tokenization\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Tokenization Comparison:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Show how 'lowest' is represented in the final vocabulary\n",
        "test_word = \"lowest\"\n",
        "print(f\"\\nOriginal word: '{test_word}'\")\n",
        "print(f\"Character-level tokens: {list(test_word)} ({len(test_word)} tokens)\")\n",
        "\n",
        "# Find the BPE representation from our final vocabulary\n",
        "for word_repr, freq in vocab.items():\n",
        "    # Check if this is the 'lowest' word (it will have </w> marker)\n",
        "    if 'l' in word_repr and 'o' in word_repr and 'w' in word_repr and 'e' in word_repr and 's' in word_repr and 't' in word_repr:\n",
        "        bpe_tokens = word_repr.replace('</w>', '').split()\n",
        "        print(f\"After BPE tokens: {bpe_tokens} ({len(bpe_tokens)} tokens)\")\n",
        "        print(f\"BPE representation: '{word_repr}'\")\n",
        "        break\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Key Insight:\")\n",
        "print(\"=\"*50)\n",
        "print(\"BPE learns subword units that balance between:\")\n",
        "print(\"  • Character-level: Flexible but creates LONG sequences\")\n",
        "print(\"  • Word-level: Compact but LIMITED vocabulary\")\n",
        "print(\"  • Subword-level (BPE): BEST OF BOTH WORLDS!\")\n",
        "print(\"\\nThis is why GPT and other modern LLMs use BPE tokenization.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPeAz32y66wxhDd+rMkVViL",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
